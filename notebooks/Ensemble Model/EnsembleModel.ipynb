{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kz3J03UnWkO2"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "hUhKQ36TXSpZ",
    "outputId": "a8d43da7-8cfa-40ba-b19f-39e6af023f96"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('marathiData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEEwiqHQXYhy"
   },
   "outputs": [],
   "source": [
    "def getStopWords():\n",
    "  with open('./stopwords-mr.txt','r') as f:\n",
    "    stopwords=f.read()\n",
    "    stopwords=stopwords.split('\\n')\n",
    "    return stopwords\n",
    "\n",
    "stopWords=getStopWords()\n",
    "\n",
    "stop_words = stopWords\n",
    "suffixes = ['ता', 'ते', 'तो', 'ल', 'ना', 'णे', 'त', 'य']\n",
    "def stem_marathi_word(word):\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "noun_suffixes = ['आणि', 'े', 'ा', 'नी', 'ची', 'मधील', 'हवे', 'ची', 'चा']\n",
    "verb_suffixes = ['त', 'तो', 'ते', 'ली', 'ला', 'ले', 'णार', 'त आहे', 'त असतील']\n",
    "def lemmatize_marathi(word):\n",
    "    # Rule-based stripping of verb suffixes\n",
    "    for suffix in verb_suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]  # Stripping the suffix\n",
    "    # Rule-based stripping of noun suffixes\n",
    "    for suffix in noun_suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "def preprocess_text(text, use_stemming=False, use_lemmatization=False):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove numbers and special characters\n",
    "    cleaned_text = ''.join(char for char in text if ('\\u0900' <= char <= '\\u097F') or char.isspace())\n",
    "\n",
    "    # Remove stop words\n",
    "    cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stop_words])\n",
    "\n",
    "    # Apply stemming or lemmatization if specified\n",
    "    if use_stemming:\n",
    "        cleaned_text = ' '.join([stem_marathi_word(word) for word in cleaned_text.split()])\n",
    "    elif use_lemmatization:\n",
    "        cleaned_text = ' '.join([lemmatize_marathi(word) for word in cleaned_text.split()])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "df['translated_source'] = df['translated_source'].apply(lambda x: preprocess_text(x))\n",
    "df['translated_plagiarism'] = df['translated_plagiarism'].apply(lambda x: preprocess_text(x))\n",
    "df['stemmed_srcText']= df['translated_source'].apply(lambda x: preprocess_text(x,use_stemming=True, use_lemmatization=True))\n",
    "df['stemmed_plagText']=df['translated_plagiarism'].apply(lambda x: preprocess_text(x,use_stemming=True, use_lemmatization=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "YOVaqlBSXecJ",
    "outputId": "06745a18-9a5b-49c4-c87e-5d18eb48ab42"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r85cv50aXfcX",
    "outputId": "81fe7a7e-dad3-482d-92b8-953b583649a9"
   },
   "outputs": [],
   "source": [
    "with open(\"bertEmbeddings.pkl\",\"rb\") as f:\n",
    "    bertEmbeddings=pickle.load(f)\n",
    "\n",
    "bertEmbeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLzvFjdhXs1_"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer400 = TfidfVectorizer(max_features=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnwwYhcyX2ar"
   },
   "outputs": [],
   "source": [
    "tfidf_embeddings_source400 = tfidf_vectorizer400.fit_transform(df['stemmed_srcText'].tolist()).toarray()\n",
    "tfidf_embeddings_plag400=tfidf_vectorizer400.fit_transform(df['stemmed_plagText'].tolist()).toarray()\n",
    "\n",
    "tfidf_embeddings400 = tfidf_embeddings_source400 - tfidf_embeddings_plag400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpNpTBhAYZvj"
   },
   "outputs": [],
   "source": [
    "def getTrainTestSplit(bertEmbeddings, tfidf_embeddings, labels):\n",
    "    # Splitting the bertEmbeddings and labels into train/test\n",
    "    bertEmbeddingsTrain = bertEmbeddings[:int(len(bertEmbeddings) * 0.8)]\n",
    "    bertEmbeddingsTest = bertEmbeddings[int(len(bertEmbeddings) * 0.8):]\n",
    "    y_train = labels[:int(len(labels) * 0.8)]\n",
    "    y_test = labels[int(len(labels) * 0.8):]\n",
    "\n",
    "    # If there are no tfidf_embeddings, return only bert embeddings\n",
    "    if len(tfidf_embeddings) == 0:\n",
    "        return bertEmbeddingsTrain, bertEmbeddingsTest, y_train, y_test\n",
    "\n",
    "    # Splitting tfidf_embeddings into train/test\n",
    "    tfidfEmbeddingsTrain = tfidf_embeddings[:int(len(tfidf_embeddings) * 0.8)]\n",
    "    tfidfEmbeddingsTest = tfidf_embeddings[int(len(tfidf_embeddings) * 0.8):]\n",
    "\n",
    "    # Concatenate the bert and tfidf embeddings for train and test\n",
    "    X_train = np.concatenate([bertEmbeddingsTrain, tfidfEmbeddingsTrain], axis=1)\n",
    "    X_test = np.concatenate([bertEmbeddingsTest, tfidfEmbeddingsTest], axis=1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, bertEmbeddingsTrain, bertEmbeddingsTest, tfidfEmbeddingsTrain, tfidfEmbeddingsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f62PKw_yY_If"
   },
   "outputs": [],
   "source": [
    "_,_,y_train, y_test,bertEmbeddingsTrain,bertEmbeddingsTest, tfidfEmbeddingsTrain, tfidfEmbeddingsTest=getTrainTestSplit(bertEmbeddings,tfidf_embeddings400,df['label'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEpsrijJH6FU"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf_classifiers = [\n",
    "    LogisticRegression(C=0.13626934782420866, n_jobs=-1),\n",
    "    LGBMClassifier(colsample_bytree=0.928627577871539,\n",
    "               learning_rate=0.18466649352827333, max_bin=15,\n",
    "               min_child_samples=12, n_estimators=1, n_jobs=-1, num_leaves=8,\n",
    "               reg_alpha=0.0019393893820794101, reg_lambda=0.15928832087494818,\n",
    "               verbose=-1),\n",
    "]\n",
    "\n",
    "bert_classifiers = [\n",
    "    XGBClassifier(base_score=None, booster=None, callbacks=[],\n",
    "              colsample_bylevel=0.1978085011113737, colsample_bynode=None,\n",
    "              colsample_bytree=0.4437517454987611, device=None,\n",
    "              early_stopping_rounds=None, enable_categorical=False,\n",
    "              eval_metric=None, feature_types=None, gamma=None,\n",
    "              grow_policy='lossguide', importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=0.16506372545399872,\n",
    "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=0, max_leaves=20,\n",
    "              min_child_weight=0.26976026917736995,\n",
    "              monotone_constraints=None, multi_strategy=None, n_estimators=371,\n",
    "              n_jobs=-1, num_parallel_tree=None, random_state=None),\n",
    "\n",
    "    SVC(\n",
    "        probability=True,\n",
    "        kernel='rbf',\n",
    "        C=100,\n",
    "        degree=2,\n",
    "        gamma='scale'\n",
    "        ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGHKMZ7la8IA"
   },
   "outputs": [],
   "source": [
    "for clf in tfidf_classifiers:\n",
    "    clf.fit(tfidfEmbeddingsTrain, y_train)\n",
    "for clf in bert_classifiers:\n",
    "    clf.fit(bertEmbeddingsTrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WL9PqBNz0237"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_weight_doublets(classifiers, embeddings, y_test):\n",
    "    doublets = []\n",
    "\n",
    "    for x in [round(i * 0.1, 1) for i in range(11)]:\n",
    "        y = round(1 - x, 1)\n",
    "        if 0 <= y <= 1:\n",
    "            doublet = (x, y)\n",
    "            doublets.append(doublet)\n",
    "\n",
    "    best_accuracy = -1\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_f1 = 0\n",
    "    best_weights = None\n",
    "\n",
    "    proba = [classifier.predict_proba(embeddings)[:, 1] for classifier in classifiers]\n",
    "\n",
    "    for weights in doublets:\n",
    "        temp = weights[0] * proba[0] + weights[1] * proba[1]\n",
    "        y_pred = (temp >= 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='binary')\n",
    "        recall = recall_score(y_test, y_pred, average='binary')\n",
    "        f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "            best_f1 = f1\n",
    "            best_weights = weights\n",
    "\n",
    "    print(f\"\\nBest Accuracy: {best_accuracy:.4f}, Precision: {best_precision:.4f}, Recall: {best_recall:.4f}, F1 Score: {best_f1:.4f}, Best Weights: {best_weights}\")\n",
    "    return best_weights\n",
    "\n",
    "def evaluate_weight_triplets(classifiers, embeddings, y_test):\n",
    "    triplets = []\n",
    "\n",
    "    for x in [round(i * 0.1, 1) for i in range(11)]:\n",
    "        for y in [round(i * 0.1, 1) for i in range(int((1 - x) * 10) + 1)]:\n",
    "            z = round(1 - x - y, 1)\n",
    "            if 0 <= z <= 1:\n",
    "                triplet = (x, y, z)\n",
    "                triplets.append(triplet)\n",
    "\n",
    "    best_accuracy = -1\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_f1 = 0\n",
    "    best_weights = None\n",
    "\n",
    "    proba = [classifier.predict_proba(embeddings)[:, 1] for classifier in classifiers]\n",
    "\n",
    "    for weights in triplets:\n",
    "        temp = weights[0] * proba[0] + weights[1] * proba[1] + weights[2] * proba[2]\n",
    "        y_pred = (temp >= 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='binary')\n",
    "        recall = recall_score(y_test, y_pred, average='binary')\n",
    "        f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "            best_f1 = f1\n",
    "            best_weights = weights\n",
    "\n",
    "    print(f\"\\nBest Accuracy: {best_accuracy:.4f}, Precision: {best_precision:.4f}, Recall: {best_recall:.4f}, F1 Score: {best_f1:.4f}, Best Weights: {best_weights}\")\n",
    "    return best_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jcSxM5zrx5d",
    "outputId": "22aa4f9a-b3ca-4356-b412-812bd301f1ac"
   },
   "outputs": [],
   "source": [
    "best_weights_for_tfidf_models=evaluate_weight_doublets(tfidf_classifiers,tfidfEmbeddingsTest,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvGPbPmH1cAW",
    "outputId": "e52eb2ad-5f0e-4bcc-bc83-9c6e7a23c4a3"
   },
   "outputs": [],
   "source": [
    "best_weights_for_bert_models=evaluate_weight_doublets(bert_classifiers,bertEmbeddingsTest,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aT3UpJUubKd4"
   },
   "outputs": [],
   "source": [
    "tfidf_predictions = np.array([\n",
    "    weight * clf.predict_proba(tfidfEmbeddingsTest)[:, 1]\n",
    "    for clf, weight in zip(tfidf_classifiers, best_weights_for_tfidf_models)\n",
    "]).sum(axis=0)\n",
    "\n",
    "bert_predictions = np.array([\n",
    "    weight * clf.predict_proba(bertEmbeddingsTest)[:, 1]\n",
    "    for clf, weight in zip(bert_classifiers, best_weights_for_bert_models)\n",
    "]).sum(axis=0)\n",
    "\n",
    "tfidf_predictions = tfidf_predictions.reshape(-1, 1)\n",
    "bert_predictions = bert_predictions.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzDME1Wzx7My"
   },
   "outputs": [],
   "source": [
    "def weighted_ensemble(tfidf_preds, bert_preds, w_tfidf, w_bert):\n",
    "    return (w_tfidf * tfidf_preds +\n",
    "            w_bert * bert_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXiKRGqQcL76"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, roc_auc_score)\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over weight combinations\n",
    "for w_tfidf in np.linspace(0, 1, 11):\n",
    "    w_bert = 1 - w_tfidf\n",
    "    # Generate combined predictions using the weighted ensemble\n",
    "    combined_predictions = weighted_ensemble(tfidf_predictions, bert_predictions, w_tfidf, w_bert)\n",
    "\n",
    "    # Convert probabilities to binary predictions (0 or 1) based on a 0.5 threshold\n",
    "    binary_predictions = (combined_predictions >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, F1 score, and AUC score\n",
    "    accuracy = accuracy_score(y_test, binary_predictions)\n",
    "    precision = precision_score(y_test, binary_predictions)\n",
    "    recall = recall_score(y_test, binary_predictions)\n",
    "    f1 = f1_score(y_test, binary_predictions)\n",
    "    auc_score = roc_auc_score(y_test, combined_predictions)\n",
    "\n",
    "    # Normalize the confusion matrix\n",
    "    cm = confusion_matrix(y_test, binary_predictions, normalize='true')\n",
    "\n",
    "    # Store metrics in the results list\n",
    "    results.append({\n",
    "        'Weight TF-IDF': w_tfidf,\n",
    "        'Weight BERT': w_bert,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'AUC Score': auc_score\n",
    "    })\n",
    "\n",
    "    # Print metrics for current weight combination\n",
    "    print(f\"w_tfidf: {w_tfidf:.3f}, w_bert: {w_bert:.3f}, \"\n",
    "          f\"Accuracy: {accuracy * 100:.3f}%, \"\n",
    "          f\"Precision: {precision * 100:.3f}%, \"\n",
    "          f\"Recall: {recall * 100:.3f}%, \"\n",
    "          f\"F1 Score: {f1 * 100:.3f}%, \"\n",
    "          f\"AUC Score: {auc_score * 100:.3f}%\")\n",
    "    print(f\"Normalized Confusion Matrix:\\n{cm}\\n\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Convert metrics to percentage format with 2 decimal places\n",
    "results_df['Accuracy'] = (results_df['Accuracy'] * 100).round(2).astype(str) + '%'\n",
    "results_df['Precision'] = (results_df['Precision'] * 100).round(2).astype(str) + '%'\n",
    "results_df['Recall'] = (results_df['Recall'] * 100).round(2).astype(str) + '%'\n",
    "results_df['F1 Score'] = (results_df['F1 Score'] * 100).round(2).astype(str) + '%'\n",
    "results_df['AUC Score'] = (results_df['AUC Score'] * 100).round(2).astype(str) + '%'\n",
    "\n",
    "# Find the best result based on accuracy\n",
    "best_result = max(results, key=lambda x: x['Accuracy'])\n",
    "best_w_tfidf = best_result['Weight TF-IDF']\n",
    "best_w_bert = best_result['Weight BERT']\n",
    "best_accuracy = best_result['Accuracy']\n",
    "best_precision = best_result['Precision']\n",
    "best_recall = best_result['Recall']\n",
    "best_f1 = best_result['F1 Score']\n",
    "best_auc_score = best_result['AUC Score']\n",
    "\n",
    "# Print best metrics\n",
    "print(\"\\nBest Result:\")\n",
    "print(f\"Best Weights -> TF-IDF: {best_w_tfidf:.3f}, BERT: {best_w_bert:.3f}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "print(f\"Best Precision: {best_precision}\")\n",
    "print(f\"Best Recall: {best_recall}\")\n",
    "print(f\"Best F1 Score: {best_f1}\")\n",
    "print(f\"Best AUC Score: {best_auc_score}\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "results_df.to_csv('ensemble_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRutXSjVchZw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
